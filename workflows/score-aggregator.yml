name: Score Aggregator

on:
  workflow_dispatch:
    inputs:
      team_name:
        description: 'Team name for the report'
        required: false
        default: 'auto-detect'
        type: string
  workflow_run:
    workflows: ["Code Quality Analysis", "Security Analysis", "Test Coverage Analysis", "Frontend Usability Analysis", "Team Behavior Analysis", "AI Prompt Quality Analysis"]
    types:
      - completed

jobs:
  aggregate-scores:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Download All Reports
      uses: actions/download-artifact@v4
      with:
        pattern: '*-report'
        path: reports/
        merge-multiple: true
      continue-on-error: true

    - name: Determine Team Name
      id: team_name
      run: |
        team_name="${{ github.event.inputs.team_name }}"
        if [ "$team_name" = "auto-detect" ] || [ -z "$team_name" ]; then
          # Try to extract team name from repo name or use repo name
          repo_name="${{ github.repository }}"
          team_name=$(echo "$repo_name" | cut -d'/' -f2)
        fi
        echo "team_name=$team_name" >> $GITHUB_OUTPUT

    - name: Aggregate All Scores
      run: |
        mkdir -p final-report
        
        # Initialize scores
        code_quality=0
        security=0
        test_coverage=0
        frontend_usability=0
        team_behavior=0
        ai_prompt_quality=0
        
        # Read individual reports
        if [ -f "reports/code-quality.json" ]; then
          code_quality=$(jq '.score // 0' reports/code-quality.json)
        fi
        
        if [ -f "reports/security.json" ]; then
          security=$(jq '.score // 0' reports/security.json)
        fi
        
        if [ -f "reports/test-coverage.json" ]; then
          test_coverage=$(jq '.score // 0' reports/test-coverage.json)
        fi
        
        if [ -f "reports/frontend-usability.json" ]; then
          frontend_usability=$(jq '.score // 0' reports/frontend-usability.json)
        fi
        
        if [ -f "reports/team-behavior.json" ]; then
          team_behavior=$(jq '.score // 0' reports/team-behavior.json)
        fi
        
        if [ -f "reports/ai-prompt-quality.json" ]; then
          ai_prompt_quality=$(jq '.score // 0' reports/ai-prompt-quality.json)
        fi
        
        # Calculate overall score (weighted average)
        overall_score=$(echo "scale=1; ($code_quality * 0.25 + $security * 0.20 + $test_coverage * 0.20 + $frontend_usability * 0.15 + $team_behavior * 0.10 + $ai_prompt_quality * 0.10)" | bc -l 2>/dev/null || echo "0")
        overall_score=${overall_score%.*}  # Remove decimal part
        
        # Create final score report
        cat > final-report/score.json << EOF
        {
          "team": "${{ steps.team_name.outputs.team_name }}",
          "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
          "repository": "${{ github.repository }}",
          "overall_score": $overall_score,
          "scores": {
            "code_quality": $code_quality,
            "security": $security,
            "test_coverage": $test_coverage,
            "frontend_usability": $frontend_usability,
            "team_behavior": $team_behavior,
            "ai_prompt_quality": $ai_prompt_quality
          },
          "weights": {
            "code_quality": 0.25,
            "security": 0.20,
            "test_coverage": 0.20,
            "frontend_usability": 0.15,
            "team_behavior": 0.10,
            "ai_prompt_quality": 0.10
          },
          "max_score": 100
        }
        EOF
        
        echo "Final Score: $overall_score/100"
        echo "Team: ${{ steps.team_name.outputs.team_name }}"

    - name: Upload Final Score Report
      uses: actions/upload-artifact@v4
      with:
        name: score-report
        path: final-report/score.json

    - name: Comment Score on PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const scoreData = JSON.parse(fs.readFileSync('final-report/score.json', 'utf8'));
          
          const comment = `## ðŸ† Hackathon Score Report
          
          **Team:** ${scoreData.team}
          **Overall Score:** ${scoreData.overall_score}/100
          
          ### Category Breakdown:
          - ðŸ§© Code Quality: ${scoreData.scores.code_quality}/100 (25%)
          - ðŸ”’ Security: ${scoreData.scores.security}/100 (20%)
          - ðŸ§ª Test Coverage: ${scoreData.scores.test_coverage}/100 (20%)
          - ðŸŽ¨ Frontend Usability: ${scoreData.scores.frontend_usability}/100 (15%)
          - ðŸ‘¥ Team Behavior: ${scoreData.scores.team_behavior}/100 (10%)
          - ðŸ¤– AI Prompt Quality: ${scoreData.scores.ai_prompt_quality}/100 (10%)
          
          Generated at: ${scoreData.timestamp}`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
